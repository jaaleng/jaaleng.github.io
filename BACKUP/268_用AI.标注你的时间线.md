# [用AI 标注你的时间线](https://github.com/jaaleng/jaaleng.github.io/issues/268)

📅 [Dayflow：AI 标注你的时间线](https://github.com/JerryZLiu/dayflow)

![](https://pic.imgdd.cc/item/69270accc828c4c6deffb220.jpg)


- AI 驱动的时间线标注：以 1 FPS 的低速率录制屏幕，每 15 分钟进行一次批量分析。它能将“在 Chrome 中花费 3 小时”等粗略指标，转化为“审查 PR 评论：45 分钟”或“调试认证流程：1.5 小时”等有意义的活动描述。
- 隐私优先的架构：用户可选择使用云端 Google Gemini API 进行高质量分析，或通过 Ollama、LM Studio 等工具在本地运行模型，实现完全离线处理，确保敏感的屏幕数据绝不离开本地设备。
- 极低的系统资源占用：应用基于原生 SwiftUI 构建，安装包约 25MB。正常运行时，内存消耗约 100MB，CPU 使用率低于 1%。
- 自动存储管理：默认自动清理超过 3 天的原始录屏文件，将磁盘占用维持在可预测的范围内（72 小时录制数据约占用 5GB）。
- 辅助生产力工具：提供分心事件高亮功能，帮助用户识别偏离任务的时刻；同时可生成工作日延时摄影，用于快速回顾一天的工作进展。

⚙️ 机制

Dayflow 将原始屏幕像素转化为语义活动摘要：

- 步骤1: 捕捉：利用 macOS 原生 ScreenCaptureKit 框架，以 1 FPS 的速率进行屏幕录制，并将视频流分割为 15 秒的片段暂存于 ~/Library/Application Support/Dayflow/ 目录。
- 步骤2: 分析：每 15 分钟，系统会收集过去 15 分钟内生成的 60 个图像帧，将其作为一个批次准备进行处理。
- 步骤3: 生成：将图像批次发送给视觉语言模型以生成结构化的 JSON 摘要。该过程根据所选模型有显著差异：云端 Gemini API 利用其原生的视频理解能力，仅需 2 次 LLM 调用；而本地模型（如通过 Ollama 运行的 Qwen2.5VL-3B）则需通过复杂的提示工程和数据分块，进行约 32 次 LLM 调用来处理相同的数据量。
- 步骤4: 显示：AI 生成的活动数据被存储在本地的 SQLite 数据库中（通过 GRDB 库管理）。前端 UI 基于 SwiftUI 构建，通过 @Observable 对象监听数据库变化，自动、响应式地更新时间线视图。
- 步骤5: 清理：一个后台进程会定期运行，默认自动删除超过三天的原始录屏文件，以控制存储空间占用。

👨🏻‍💻 使用场景

- 个人生产力与自我认知：知识工作者可用于实时了解自己的时间分配情况，通过即时反馈来调整工作节奏、对抗拖延，并确保工作与计划保持一致。
- 专业计费与时间追踪：适用于律师、顾问、自由职业者等按时计费的专业人士。其自动活动捕捉功能可记录所有可计费工作，有效避免因手动记录不完整而造成的收入损失。
- ADHD 与注意力管理：为 ADHD 等神经多样性用户提供任务完成度的可视化反馈和激励，帮助他们更好地理解自身的注意力模式，并建立更准确的时间感。
- 开发与研究流程优化：开发者和研究人员可利用它来分析自身的工作流程，识别深度工作时段、上下文切换的频率和成本，以及常见的中断模式，从而优化个人或团队的工作安排。

